A cutting-edge approach that combines advanced computer vision techniques with
speech synthesis capabilities to achieve groundbreaking results. The impact of this
innovation extends beyond technical advancements, finding relevance in diverse realtime applications. The fusion of speech synthesis with object detection adds a layer
of accessibility and convenience to multiple scenarios. This project focus on combining speech and vision modalities to yield accurate object detection and distance
calculation. This innovative endeavor sets the stage for intelligent systems that not
only visualize the world but also communicate findings audibly, opening doors to
novel applications and possibilities across various sectors.This system can revolutionize the way individuals with visual impairments interact with their surroundings,
offering an auditory understanding of their environment. Over the past few years they
have witnessed the success of convolution neural network (CNN) for visual object
detection. To represent objects of various appearance, aspect ratios and poses with
limited convolutional features, many CNN-based detectors leverage features boxes
as reference points for object localization. Each assigned feature independently convolutional network learning for object prediction, based upon the assumption that
the features spatially aligned with objects are always appropriate for classification
and localization. A spatially aligned features might correspond to less representative features, which determines classification and localization performance. On the
other hand, it is impossible to match objects with proper features using the IoU criterion when multiple objects come together. The project aims to improve object
detection accuracy by 92% by incorporating multimodal data (visual and audio) and
distance calculation using convolutional neural networks. This enhances localization, robustness, and potential applications in real-time scenarios like surveillance
and autonomous systems.
